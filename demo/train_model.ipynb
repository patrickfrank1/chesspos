{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import chess\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from chesspos.utils import board_to_bitboard,bitboard_to_board\n",
    "from chesspos.preprocessing import SampleGenerator\n",
    "from chesspos.models import DenseAutoencoder, CnnAutoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 773)]             0         \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 8, 8, 12)          0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 8, 8, 32)          6176      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 4, 4, 64)          8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "=================================================================\n",
      "Total params: 55,584\n",
      "Trainable params: 55,584\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 4, 4, 32)          8224      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 8, 8, 12)          6156      \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 768)               0         \n",
      "=================================================================\n",
      "Total params: 55,724\n",
      "Trainable params: 55,724\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"autocoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 773)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            (None, 64)           55584       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, 768)          55724       encoder[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 5)            0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 773)          0           decoder[0][0]                    \n",
      "                                                                 lambda_1[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 111,308\n",
      "Trainable params: 111,308\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 773)]             0         \n",
      "_________________________________________________________________\n",
      "lambda_2 (Lambda)            (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 8, 8, 12)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 32)          6176      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 64)          8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                8256      \n",
      "=================================================================\n",
      "Total params: 55,584\n",
      "Trainable params: 55,584\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "reshape_4 (Reshape)          (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 4, 4, 32)          8224      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 8, 8, 12)          6156      \n",
      "_________________________________________________________________\n",
      "reshape_5 (Reshape)          (None, 768)               0         \n",
      "=================================================================\n",
      "Total params: 55,724\n",
      "Trainable params: 55,724\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"autocoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 773)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            (None, 64)           55584       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, 768)          55724       encoder[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 5)            0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 773)          0           decoder[0][0]                    \n",
      "                                                                 lambda_3[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 111,308\n",
      "Trainable params: 111,308\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "You have enough training samples for 69.028125 epochs and enought validation samples for 225.6203125 epochs.\n",
      "\n",
      "Training on 0.22089 million training samples.\n",
      "Validating on 0.04416 million validation samples.\n",
      "WARNING: your are providing much more validation samples than nessecary. Those could be used for training instead.\n",
      "Epoch 1/69\n",
      "100/100 [==============================] - 8s 67ms/step - loss: 0.1952 - val_loss: 0.1621\n",
      "Epoch 2/69\n",
      "100/100 [==============================] - 1s 15ms/step - loss: 0.1619 - val_loss: 0.1422\n",
      "Epoch 3/69\n",
      "100/100 [==============================] - 1s 15ms/step - loss: 0.1291 - val_loss: 0.1150\n",
      "Epoch 4/69\n",
      "100/100 [==============================] - 1s 14ms/step - loss: 0.1116 - val_loss: 0.1144\n",
      "Epoch 5/69\n",
      "100/100 [==============================] - 1s 14ms/step - loss: 0.1101 - val_loss: 0.1160\n",
      "Epoch 6/69\n",
      "100/100 [==============================] - 1s 15ms/step - loss: 0.1107 - val_loss: 0.1118\n",
      "Epoch 7/69\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1099 - val_loss: 0.1115\n",
      "Epoch 8/69\n",
      "100/100 [==============================] - 1s 15ms/step - loss: 0.1084 - val_loss: 0.1087\n",
      "Epoch 9/69\n",
      "100/100 [==============================] - 1s 14ms/step - loss: 0.1064 - val_loss: 0.1082\n",
      "Epoch 10/69\n",
      "100/100 [==============================] - 1s 14ms/step - loss: 0.1069 - val_loss: 0.1058\n",
      "Epoch 11/69\n",
      "100/100 [==============================] - 2s 15ms/step - loss: 0.1069 - val_loss: 0.1049\n",
      "Epoch 12/69\n",
      "100/100 [==============================] - 1s 15ms/step - loss: 0.1047 - val_loss: 0.1048\n",
      "Epoch 13/69\n",
      "100/100 [==============================] - 1s 15ms/step - loss: 0.1045 - val_loss: 0.1096\n",
      "Epoch 14/69\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.1022 - val_loss: 0.1025\n",
      "Epoch 15/69\n",
      "100/100 [==============================] - 1s 14ms/step - loss: 0.1037 - val_loss: 0.1037\n",
      "Epoch 16/69\n",
      "100/100 [==============================] - 2s 15ms/step - loss: 0.1053 - val_loss: 0.1056\n",
      "Epoch 17/69\n",
      "100/100 [==============================] - 1s 14ms/step - loss: 0.1028 - val_loss: 0.1034\n",
      "Epoch 18/69\n",
      "100/100 [==============================] - 1s 15ms/step - loss: 0.1038 - val_loss: 0.1017\n",
      "Epoch 19/69\n",
      "100/100 [==============================] - 1s 14ms/step - loss: 0.1044 - val_loss: 0.1027\n",
      "Epoch 20/69\n",
      "100/100 [==============================] - 1s 14ms/step - loss: 0.1023 - val_loss: 0.1034\n",
      "Epoch 21/69\n",
      "100/100 [==============================] - 1s 14ms/step - loss: 0.1049 - val_loss: 0.1019\n",
      "Epoch 22/69\n",
      "100/100 [==============================] - 1s 14ms/step - loss: 0.1020 - val_loss: 0.1046\n",
      "Epoch 23/69\n",
      "100/100 [==============================] - 1s 15ms/step - loss: 0.1009 - val_loss: 0.0950\n",
      "Epoch 24/69\n",
      "100/100 [==============================] - 2s 15ms/step - loss: 0.0993 - val_loss: 0.0970\n",
      "Epoch 25/69\n",
      "100/100 [==============================] - 1s 14ms/step - loss: 0.0942 - val_loss: 0.0951\n",
      "Epoch 26/69\n",
      "100/100 [==============================] - 1s 14ms/step - loss: 0.0963 - val_loss: 0.0931\n",
      "Epoch 27/69\n",
      "100/100 [==============================] - 1s 14ms/step - loss: 0.0946 - val_loss: 0.0917\n",
      "Epoch 28/69\n",
      "100/100 [==============================] - 1s 14ms/step - loss: 0.0920 - val_loss: 0.0928\n",
      "Epoch 29/69\n",
      "100/100 [==============================] - 2s 24ms/step - loss: 0.0906 - val_loss: 0.0942\n",
      "Epoch 30/69\n",
      "100/100 [==============================] - 1s 14ms/step - loss: 0.0883 - val_loss: 0.0877\n",
      "Epoch 31/69\n",
      "100/100 [==============================] - 2s 17ms/step - loss: 0.0881 - val_loss: 0.0898\n",
      "Epoch 32/69\n",
      "100/100 [==============================] - 2s 15ms/step - loss: 0.0885 - val_loss: 0.0904\n",
      "Epoch 33/69\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.0920 - val_loss: 0.0843\n",
      "Epoch 34/69\n",
      "100/100 [==============================] - 2s 17ms/step - loss: 0.0892 - val_loss: 0.0882\n",
      "Epoch 35/69\n",
      "100/100 [==============================] - 11s 112ms/step - loss: 0.0864 - val_loss: 0.0881\n",
      "Epoch 36/69\n",
      "100/100 [==============================] - 2s 21ms/step - loss: 0.0906 - val_loss: 0.0882\n",
      "Epoch 37/69\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.0886 - val_loss: 0.0928\n",
      "Epoch 38/69\n",
      "100/100 [==============================] - 2s 19ms/step - loss: 0.0905 - val_loss: 0.0897\n",
      "Epoch 39/69\n",
      "100/100 [==============================] - 2s 15ms/step - loss: 0.0871 - val_loss: 0.0957\n",
      "Epoch 40/69\n",
      "100/100 [==============================] - 1s 14ms/step - loss: 0.0871 - val_loss: 0.0843\n",
      "Epoch 41/69\n",
      "100/100 [==============================] - 1s 15ms/step - loss: 0.0894 - val_loss: 0.0901\n",
      "Epoch 42/69\n",
      "100/100 [==============================] - 2s 18ms/step - loss: 0.0894 - val_loss: 0.0868\n",
      "Epoch 43/69\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.0908 - val_loss: 0.0867\n",
      "Epoch 44/69\n",
      "100/100 [==============================] - 1s 15ms/step - loss: 0.0898 - val_loss: 0.0897\n",
      "Epoch 45/69\n",
      "100/100 [==============================] - 1s 15ms/step - loss: 0.0904 - val_loss: 0.0917\n",
      "Epoch 46/69\n",
      "100/100 [==============================] - 2s 15ms/step - loss: 0.0894 - val_loss: 0.0872\n",
      "Epoch 47/69\n",
      "100/100 [==============================] - 1s 14ms/step - loss: 0.0915 - val_loss: 0.0872\n",
      "Epoch 48/69\n",
      "100/100 [==============================] - 1s 15ms/step - loss: 0.0886 - val_loss: 0.0880\n",
      "Epoch 49/69\n",
      "100/100 [==============================] - 1s 14ms/step - loss: 0.0870 - val_loss: 0.0875\n",
      "Epoch 50/69\n",
      "100/100 [==============================] - 1s 15ms/step - loss: 0.0912 - val_loss: 0.0936\n",
      "Epoch 51/69\n",
      "100/100 [==============================] - 1s 14ms/step - loss: 0.0898 - val_loss: 0.0890\n",
      "Epoch 52/69\n",
      "100/100 [==============================] - 2s 15ms/step - loss: 0.0895 - val_loss: 0.0906\n",
      "Epoch 53/69\n",
      "100/100 [==============================] - 1s 14ms/step - loss: 0.0916 - val_loss: 0.0936\n",
      "Epoch 54/69\n",
      "100/100 [==============================] - 1s 14ms/step - loss: 0.0929 - val_loss: 0.0915\n",
      "Epoch 55/69\n",
      "100/100 [==============================] - 2s 17ms/step - loss: 0.0902 - val_loss: 0.0932\n",
      "Epoch 56/69\n",
      "100/100 [==============================] - 2s 15ms/step - loss: 0.0906 - val_loss: 0.0947\n",
      "Epoch 57/69\n",
      "100/100 [==============================] - 1s 15ms/step - loss: 0.0924 - val_loss: 0.0877\n",
      "Epoch 58/69\n",
      "100/100 [==============================] - 1s 15ms/step - loss: 0.0917 - val_loss: 0.0886\n",
      "Epoch 59/69\n",
      "100/100 [==============================] - 1s 14ms/step - loss: 0.0909 - val_loss: 0.0873\n",
      "Epoch 60/69\n",
      "100/100 [==============================] - 2s 17ms/step - loss: 0.0905 - val_loss: 0.0917\n",
      "Epoch 61/69\n",
      "100/100 [==============================] - 2s 17ms/step - loss: 0.0901 - val_loss: 0.0933\n",
      "Epoch 62/69\n",
      "100/100 [==============================] - 1s 15ms/step - loss: 0.0906 - val_loss: 0.0896\n",
      "Epoch 63/69\n",
      "100/100 [==============================] - 3s 25ms/step - loss: 0.0912 - val_loss: 0.0941\n",
      "Epoch 64/69\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.0930 - val_loss: 0.0976\n",
      "Epoch 65/69\n",
      "100/100 [==============================] - 1s 14ms/step - loss: 0.0918 - val_loss: 0.0933\n",
      "Epoch 66/69\n",
      "100/100 [==============================] - 1s 14ms/step - loss: 0.0914 - val_loss: 0.0914\n",
      "Epoch 67/69\n",
      "100/100 [==============================] - 1s 14ms/step - loss: 0.0912 - val_loss: 0.0907\n",
      "Epoch 68/69\n",
      "100/100 [==============================] - 1s 15ms/step - loss: 0.0912 - val_loss: 0.0902\n",
      "Epoch 69/69\n",
      "100/100 [==============================] - 1s 14ms/step - loss: 0.0926 - val_loss: 0.0961\n",
      "INFO:tensorflow:Assets written to: /home/pafrank/Documents/coding/chess-position-embedding/metric_learning/new_autoencoder_test//model/assets\n"
     ]
    }
   ],
   "source": [
    "train_generator = SampleGenerator(\n",
    "    sample_dir = \"/home/pafrank/Documents/coding/chess-position-embedding/data/train_medium/\",\n",
    "    batch_size = 32\n",
    ")\n",
    "train_generator.set_subsampling_functions(['singlets'])\n",
    "train_generator.construct_generator()\n",
    "\n",
    "test_generator = SampleGenerator(\n",
    "    sample_dir = \"/home/pafrank/Documents/coding/chess-position-embedding/data/validation_medium/\",\n",
    "    batch_size = 32\n",
    ")\n",
    "test_generator.set_subsampling_functions(['singlets'])\n",
    "test_generator.construct_generator()\n",
    "\n",
    "autoencoder = CnnAutoencoder(\n",
    "    input_size = 773,\n",
    "    embedding_size = 64,\n",
    "    # hidden_layers = [1025, 0.3, 1024, 0.3, 512, 0.3, 512, 0.3, 256, 0.3, 256, 0.3, 256, 0.3, 128, 0.3, 128, 0.3, 128, 0.3, 128, 0.3],\n",
    "    hidden_layers = [512, 0.3, 256, 0.3, 128, 0.3],\n",
    "    loss = 'binary_crossentropy',\n",
    "    train_generator = train_generator,\n",
    "    test_generator = test_generator,\n",
    "\tsafe_dir = \"/home/pafrank/Documents/coding/chess-position-embedding/metric_learning/new_autoencoder_test/\",\n",
    "    train_steps_per_epoch = 100,\n",
    "    test_steps_per_epoch = 20,\n",
    "    tf_callbacks = [\n",
    "        'checkpoints',\n",
    "        keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.05, patience=10, verbose=0, mode='min', restore_best_weights=True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "autoencoder.build_model()\n",
    "autoencoder.compile()\n",
    "history = autoencoder.train()\n",
    "autoencoder.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'train_loss'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8d81075932f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/pafrank/Documents/coding/chess-position-embedding/metric_learning/new_autoencoder_test/train_loss.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'train_loss'"
     ]
    }
   ],
   "source": [
    "plt.plot(history.history['train_loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.savefig(\"/home/pafrank/Documents/coding/chess-position-embedding/metric_learning/new_autoencoder_test/train_loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.024102799594402313\n",
      "Original:          Reconstructed:\n",
      "r . b q k . n r    r . b q k . \u001b[31m.\u001b[0m r\n",
      "p p . . p p b p    p p . . p p b p\n",
      ". . n p . . p .    . . \u001b[31m.\u001b[0m \u001b[31m.\u001b[0m . . p .\n",
      ". . . . . . . .    . . . . . . . .\n",
      ". . . P . . . .    . . . \u001b[31m.\u001b[0m . . . .\n",
      ". . N B P N . .    . . \u001b[31m.\u001b[0m \u001b[31m.\u001b[0m \u001b[31m.\u001b[0m N . .\n",
      "P P . . . P P P    P P \u001b[31mP\u001b[0m . . P P P\n",
      "k . B Q K . . R    k . B Q K . . R\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_samples = autoencoder.get_best_samples(5, 2)\n",
    "best_sample_bitboard = best_samples[0]['position'].reshape((773, -1))\n",
    "best_sample_position = bitboard_to_board(best_sample_bitboard)\n",
    "print(best_samples[0]['loss'])\n",
    "autoencoder.compare_sample_to_prediction(best_sample_bitboard)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.16726478934288025\n",
      "Original:          Reconstructed:\n",
      "r . . . k . . r    r . . . \u001b[31m.\u001b[0m . . r\n",
      ". . . . . p p .    . . . . . p p .\n",
      "p q p p . b b p    \u001b[31m.\u001b[0m \u001b[31m.\u001b[0m \u001b[31m.\u001b[0m \u001b[31m.\u001b[0m . \u001b[31m.\u001b[0m \u001b[31m.\u001b[0m \u001b[31m.\u001b[0m\n",
      ". p . . p . . .    . \u001b[31m.\u001b[0m . . \u001b[31m.\u001b[0m . . .\n",
      ". Q . . P . P .    . \u001b[31m.\u001b[0m . . \u001b[31m.\u001b[0m . \u001b[31m.\u001b[0m .\n",
      ". P . P . N . P    . \u001b[31m.\u001b[0m . \u001b[31m.\u001b[0m . \u001b[31m.\u001b[0m . \u001b[31m.\u001b[0m\n",
      "N P P . . P . .    \u001b[31mP\u001b[0m P \u001b[31m.\u001b[0m . . P \u001b[31mP\u001b[0m \u001b[31mP\u001b[0m\n",
      "k K . R . . . R    \u001b[31mr\u001b[0m \u001b[31m.\u001b[0m . \u001b[31m.\u001b[0m . . . \u001b[31m.\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "worst_samples = autoencoder.get_worst_samples(5, 2)\n",
    "worst_sample_bitboard = worst_samples[0]['position'].reshape((773, -1))\n",
    "worst_sample_position = bitboard_to_board(worst_sample_bitboard)\n",
    "print(worst_samples[0]['loss'])\n",
    "autoencoder.compare_sample_to_prediction(worst_sample_bitboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "r . . . r . . .\np p q . n p k p\n. . p . . . p .\n. . n . p . . P\n. . . . P . b .\n. . . . . N P .\nP P P Q . P B .\nR N . . R . K .\n"
     ]
    }
   ],
   "source": [
    "fen = \"r3r3/ppq1npkp/2p3p1/2n1p2P/4P1b1/5NP1/PPPQ1PB1/RN2R1K1 w - - 1 15\"\n",
    "board = chess.Board(fen)\n",
    "bitboard = board_to_bitboard(board)\n",
    "bitboard = bitboard.reshape((-1,773))\n",
    "bitboard.shape\n",
    "print(board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n  0.         0.09854125 0.07537822 0.         0.         1.977425\n  1.1287615  0.         1.1234765  2.5128677  0.         0.\n  0.         0.         0.         0.06619611 0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.8238183  0.         0.         0.12656721 0.\n  0.         0.         0.         0.         0.26006028 0.\n  0.21432841 0.         0.07493638 0.10044803 0.         0.\n  0.34139058 0.         0.08777086 0.10652842 0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.2474949  0.         0.         2.168271  ]]\n"
     ]
    }
   ],
   "source": [
    "position_embedding = autoencoder.get_embedding_of_fen(fen)\n",
    "print(position_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1, 773) float32\nr . . . . . . .\np p . . . p p p\n. . . . . . . .\n. . . . . . . .\n. . . . . . . .\n. . . . . . . .\nP P . . . P P P\nr . . . . . K .\n"
     ]
    }
   ],
   "source": [
    "reconstructed_board = simple_autoencoder.get_board_of_embedding(position_embedding)\n",
    "print(reconstructed_board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 8, 8, 12) dtype=float32 (created by layer 'conv2d_transpose_1')>"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "test_generator = SampleGenerator(\n",
    "    sample_dir = \"/home/pafrank/Documents/coding/chess-position-embedding/data/validation_small/\",\n",
    "    batch_size = 16\n",
    ")\n",
    "test_generator.set_subsampling_functions(['singlets'])\n",
    "test_generator.construct_generator()\n",
    "generator = test_generator.get_generator()\n",
    "x, y = generator.__next__()\n",
    "x = tf.random.normal((16,773))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"encoder\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_3 (InputLayer)         [(None, 773)]             0         \n_________________________________________________________________\nlambda_2 (Lambda)            (None, 768)               0         \n_________________________________________________________________\nreshape_3 (Reshape)          (None, 8, 8, 12)          0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 8, 8, 32)          6176      \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 4, 4, 32)          0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 4, 4, 64)          8256      \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 2, 2, 64)          0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 256)               0         \n=================================================================\nTotal params: 14,432\nTrainable params: 14,432\nNon-trainable params: 0\n_________________________________________________________________\nModel: \"decoder\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_4 (InputLayer)         [(None, 256)]             0         \n_________________________________________________________________\nreshape_4 (Reshape)          (None, 2, 2, 64)          0         \n_________________________________________________________________\nconv2d_transpose_2 (Conv2DTr (None, 4, 4, 32)          8224      \n_________________________________________________________________\nconv2d_transpose_3 (Conv2DTr (None, 8, 8, 12)          6156      \n_________________________________________________________________\nreshape_5 (Reshape)          (None, 768)               0         \n=================================================================\nTotal params: 14,380\nTrainable params: 14,380\nNon-trainable params: 0\n_________________________________________________________________\nModel: \"autocoder\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_3 (InputLayer)            [(None, 773)]        0                                            \n__________________________________________________________________________________________________\nencoder (Functional)            (None, 256)          14432       input_3[0][0]                    \n__________________________________________________________________________________________________\ndecoder (Functional)            (None, 768)          14380       encoder[0][0]                    \n__________________________________________________________________________________________________\nlambda_3 (Lambda)               (None, 5)            0           input_3[0][0]                    \n__________________________________________________________________________________________________\nconcatenate (Concatenate)       (None, 773)          0           decoder[0][0]                    \n                                                                 lambda_3[0][0]                   \n==================================================================================================\nTotal params: 28,812\nTrainable params: 28,812\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_layer = layers.Input(shape=773, dtype=tf.float32)\n",
    "\n",
    "board_layer = layers.Lambda(lambda x: x[:,:768], output_shape=(768,))(input_layer)\n",
    "board_layer = layers.Reshape((8,8,12))(board_layer)\n",
    "\n",
    "metadata_layer = layers.Lambda(lambda x: x[:,768:], output_shape=(5,))(input_layer)\n",
    "\n",
    "encoder = layers.Conv2D(32, (4, 4), activation=\"relu\", padding=\"same\")(board_layer)\n",
    "encoder = layers.MaxPooling2D((2, 2), padding=\"same\")(encoder)\n",
    "encoder = layers.Conv2D(64, (2, 2), activation=\"relu\", padding=\"same\")(encoder)\n",
    "encoder = layers.MaxPooling2D((2, 2), padding=\"same\")(encoder)\n",
    "encoder = layers.Flatten()(encoder)\n",
    "\n",
    "encoder_model = keras.Model(inputs=input_layer, outputs=encoder, name='encoder')\n",
    "encoder_model.summary()\n",
    "\n",
    "decoder_input = layers.Input(shape=256)\n",
    "decoder = layers.Reshape((2,2,64))(decoder_input)\n",
    "decoder = layers.Conv2DTranspose(32, (2, 2), strides=2, activation=\"relu\", padding=\"same\")(decoder)\n",
    "decoder = layers.Conv2DTranspose(12, (4, 4), strides=2, activation=\"relu\", padding=\"same\")(decoder)\n",
    "decoder = layers.Reshape((768,))(decoder)\n",
    "\n",
    "decoder_model = keras.Model(inputs=decoder_input, outputs=decoder, name='decoder')\n",
    "decoder_model.summary()\n",
    "\n",
    "autoencoder = encoder_model(input_layer)\n",
    "autoencoder = decoder_model(autoencoder)\n",
    "output_layer = layers.Concatenate()([autoencoder, metadata_layer])\n",
    "\n",
    "autoencoder_model = keras.Model(inputs=input_layer, outputs=output_layer, name='autocoder')\n",
    "autoencoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}